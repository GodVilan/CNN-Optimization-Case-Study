# CNN Optimization Case Study  
## Architecture vs Optimizer Impact on CIFAR-10  

**Author:** Srikant Reddy Nandireddy  
Graduate Student â€“ Data Science & AI  

---

## ğŸ“Œ Overview

This project investigates how architectural depth and optimizer selection impact Convolutional Neural Network (CNN) performance on the CIFAR-10 dataset.

Rather than simply increasing model complexity, this study evaluates how training dynamics and optimization strategy influence generalization.

---

## ğŸ¯ Research Questions

1. Does increasing CNN depth improve performance?  
2. How significantly does optimizer choice affect convergence and generalization?  
3. How can overfitting be diagnosed using validation behavior?  

---

## ğŸ“Š Dataset

- CIFAR-10  
- 60,000 RGB images (32Ã—32)  
- 10 object classes  
- 50,000 training samples  
- 10,000 test samples  

---

## ğŸ— Model Architectures

### Baseline CNN
- Conv(32) â†’ MaxPooling  
- Dense(128)  
- Softmax Output  

### Deep CNN
- 3 Convolutional Blocks (32 â†’ 64 â†’ 128 filters)  
- Dropout Regularization  
- Fully Connected Layers (1024 â†’ 512)  
- Softmax Output  

**Total parameters:** ~2.9M  

---

## ğŸ§ª Experiments & Results

| Model | Optimizer | Test Accuracy |
|-------|-----------|--------------|
| Baseline CNN | SGD | **60.05%** |
| Deep CNN | SGD | **69.90%** |
| Deep CNN | Adam | **77.04%** |

---

## ğŸ” Key Findings

- Increasing architectural depth improved representational capacity (+9.85% over baseline).  
- Deep CNN trained with SGD exhibited validation divergence, indicating overfitting.  
- Replacing SGD with Adam improved performance by over 7%.  
- Optimization strategy had a larger impact on generalization than architectural complexity alone.  

---

## ğŸ“ˆ Training Analysis

Validation curves revealed:

- Training accuracy continued increasing.  
- Validation accuracy plateaued when using SGD.  
- Adam provided smoother convergence and better stability.  

This highlights the importance of monitoring validation behavior rather than relying solely on training accuracy.

---

## ğŸ›  Technologies Used

- Python  
- TensorFlow / Keras  
- NumPy  
- Matplotlib  

---

## ğŸ§  Conclusion

This case study demonstrates that CNN performance is strongly influenced by optimization dynamics and training configuration.

While architectural depth increases representational power, effective generalization depends on:

- Optimizer selection  
- Validation monitoring  
- Controlled experimentation  
- Overfitting diagnosis  

Deep learning performance is driven by disciplined experimentation â€” not just stacking additional layers.

---

## ğŸ“¬ Contact

**Srikant Reddy Nandireddy**  
Graduate Student â€“ Data Science & AI  
Open to Data Science / Machine Learning opportunities  